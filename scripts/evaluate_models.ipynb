{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JwOU4JoBbBq"
      },
      "source": [
        "# Evaluación de Modelos de Lenguaje - OWASP Generative API\n",
        "Este notebook está diseñado para evaluar el rendimiento de tres modelos de lenguaje en el contexto de preguntas y respuestas sobre OWASP Top 10. Los modelos evaluados son:\n",
        "\n",
        "1. bloom\n",
        "2. gpt-neo-2.7B\n",
        "3. Flan-T5-Base\n",
        "\n",
        "Se utilizarán métricas avanzadas como Exact Match, BLEU y ROUGE para evaluar la calidad de las respuestas generadas. Además, se medirá la latencia y el tiempo total de evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msuZYfFyBbBs",
        "outputId": "45dd43be-acbd-49c1-8ecb-8ae7760d0907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "# Instalación de dependencias necesarias\n",
        "!pip install transformers torch tqdm nltk rouge-score python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5718uZcIBbBt"
      },
      "source": [
        "## Configuración inicial del notebook\n",
        "Configuramos el entorno para que soporte la evaluación de los modelos. Esto incluye la carga de librerías y la configuración del dataset de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tl3AT8G2BbBt"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import os\n",
        "from transformers import pipeline\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jiey51iyGar8"
      },
      "outputs": [],
      "source": [
        "load_dotenv('.env')\n",
        "\n",
        "hf_token = os.environ['HF_TOKEN']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASURNda7G4zB"
      },
      "source": [
        "Configuramos el token para Hugging Face.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JlhS78YG8m5",
        "outputId": "495486c4-1d36-4554-fcb4-cd3ef84720fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "hf_token = os.environ['HF_TOKEN']\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLccL1T5BbBt"
      },
      "source": [
        "### Subida del dataset de prueba\n",
        "Subimos el archivo `test_questions.json` que contiene las preguntas y respuestas esperadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "8HfQZqIFBbBt",
        "outputId": "ab5b9e92-912a-4603-9328-46e79e2243f5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a0dbe7dd-d919-4485-88ff-41b9c56ac329\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a0dbe7dd-d919-4485-88ff-41b9c56ac329\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_questions.json to test_questions (1).json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "dataset_file = 'test_questions.json'\n",
        "\n",
        "with open(dataset_file, 'r') as f:\n",
        "    test_questions = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "VEL5L7m-ZVi1",
        "outputId": "b23b9697-7818-4737-fb2d-ab44d1f55d4a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-096834f7-9c39-4d14-a34b-7cd0a7bf1ad2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-096834f7-9c39-4d14-a34b-7cd0a7bf1ad2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_questions_categories.json to test_questions_categories.json\n",
            "Dataset cargado con 3 preguntas.\n"
          ]
        }
      ],
      "source": [
        "dataset_file_2 = 'test_questions_categories.json'\n",
        "\n",
        "uploaded = files.upload()\n",
        "dataset_file = 'test_questions_categories.json'\n",
        "\n",
        "with open(dataset_file_2, 'r') as f:\n",
        "    test_questions_categories = json.load(f)\n",
        "\n",
        "print(f'Dataset cargado con {len(test_questions_categories)} preguntas.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuXKyp8EBbBu"
      },
      "source": [
        "## Funciones para la evaluación de modelos\n",
        "Se define una función `evaluate_model` para realizar la evaluación de cada modelo, calculando las métricas Exact Match, BLEU y ROUGE, además de medir la latencia de las respuestas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ATdpS3EWBbBu"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from transformers import pipeline\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def evaluate_text_generation_model(model_name, test_questions):\n",
        "    \"\"\"\n",
        "    Evalúa un modelo de generación de texto en base a un conjunto de prompts con contexto y respuestas esperadas.\n",
        "    \"\"\"\n",
        "    print(f'\\nEvaluando modelo: {model_name}')\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Cargar el modelo\n",
        "    text_gen_pipeline = pipeline('text-generation', model=model_name, tokenizer=model_name)\n",
        "\n",
        "    # Inicializar métricas\n",
        "    total_bleu = 0\n",
        "    total_rouge1 = 0\n",
        "    total_rougeL = 0\n",
        "    latencies = []\n",
        "\n",
        "    # Inicializar evaluador ROUGE\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    results = []\n",
        "\n",
        "    for test in test_questions:\n",
        "        question = test['question']\n",
        "        context = test['expected']\n",
        "        prompt = f\"{question}\"\n",
        "\n",
        "        # Medir latencia\n",
        "        start = time.time()\n",
        "        result = text_gen_pipeline(\n",
        "            prompt,\n",
        "            max_length=80,\n",
        "            do_sample=True,\n",
        "            temperature=0.5,\n",
        "            top_k=50,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "        latency = time.time() - start\n",
        "        latencies.append(latency)\n",
        "\n",
        "        generated = result[0]['generated_text'] if isinstance(result, list) else result['generated_text']\n",
        "        print(f'\\nPregunta: {question}')\n",
        "        print(f'Respuesta generada: {generated}')\n",
        "        print(f'Respuesta esperada: {context}')\n",
        "        print(f'Latencia: {latency:.4f} segundos')\n",
        "\n",
        "        # BLEU Score\n",
        "        bleu_score = sentence_bleu([context.split()], generated.split())\n",
        "        total_bleu += bleu_score\n",
        "\n",
        "        # ROUGE Scores\n",
        "        rouge_scores = rouge.score(context, generated)\n",
        "        total_rouge1 += rouge_scores['rouge1'].fmeasure\n",
        "        total_rougeL += rouge_scores['rougeL'].fmeasure\n",
        "\n",
        "        # Almacenar resultados individuales\n",
        "        results.append({\n",
        "            'question': question,\n",
        "            'expected': context,\n",
        "            'generated': generated,\n",
        "            'latency': latency,\n",
        "            'bleu': bleu_score,\n",
        "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
        "            'rougeL': rouge_scores['rougeL'].fmeasure\n",
        "        })\n",
        "\n",
        "    # Promedios\n",
        "    avg_bleu = total_bleu / len(test_questions)\n",
        "    avg_rouge1 = total_rouge1 / len(test_questions)\n",
        "    avg_rougeL = total_rougeL / len(test_questions)\n",
        "    avg_latency = sum(latencies) / len(latencies)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Resultados finales\n",
        "    final_results = {\n",
        "        'model_name': model_name,\n",
        "        'avg_bleu': avg_bleu,\n",
        "        'avg_rouge1': avg_rouge1,\n",
        "        'avg_rougeL': avg_rougeL,\n",
        "        'avg_latency': avg_latency,\n",
        "        'total_time': total_time,\n",
        "        'details': results\n",
        "    }\n",
        "\n",
        "    print(f'\\n--- Resultados del modelo {model_name} ---')\n",
        "    print(f'BLEU Promedio: {avg_bleu:.4f}')\n",
        "    print(f'ROUGE-1 Promedio: {avg_rouge1:.4f}')\n",
        "    print(f'ROUGE-L Promedio: {avg_rougeL:.4f}')\n",
        "    print(f'Latencia Promedio: {avg_latency:.4f} segundos')\n",
        "    print(f'Tiempo Total: {total_time:.2f} segundos\\n')\n",
        "\n",
        "    return final_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye6VA675YMFW"
      },
      "source": [
        "Complementario a esto, se realiza una función adicional `evaluate_model_categories` para probar el dataset complementario el cual evalúa 3 categorías: robustez, respuestas abiertas, y multilingüismo por separado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IrI15y1hY2kk"
      },
      "outputs": [],
      "source": [
        "def evaluate_models_with_categories(model_name, test_categories):\n",
        "    \"\"\"\n",
        "    Evalúa un modelo de generación de texto en base a categorías de preguntas.\n",
        "    \"\"\"\n",
        "    print(f'\\nEvaluando modelo: {model_name} por categorías')\n",
        "    all_results = {}\n",
        "\n",
        "    for category in test_categories:\n",
        "        category_name = category['category']\n",
        "        print(f'\\nCategoría: {category_name}')\n",
        "\n",
        "        # Evaluar cada categoría\n",
        "        category_results = evaluate_text_generation_model(model_name, category['questions'])\n",
        "        all_results[category_name] = category_results\n",
        "\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odgcdNvhBbBu"
      },
      "source": [
        "### Evaluación de los modelos seleccionados\n",
        "Se evalúan los siguientes modelos:\n",
        "\n",
        "1. bloom\n",
        "2. gpt-neo-2.7B\n",
        "3. Flan-T5-Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tw668IkBbBv",
        "outputId": "7fcdc7a2-422a-4515-b967-5f97c3426dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluando modelo: bigscience/bloom-560m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: ¿Qué es una inyección SQL?\n",
            "Respuesta generada: ¿Qué es una inyección SQL? ¿Es una inyección SQL? – Stack Overflow en español\n",
            "Una inyección SQL es una forma de hacer que un programa que se ejecuta en un servidor web se ejecute en otro servidor web. Este tipo de inyección es más comúnmente utilizada para hacer que un programa se ejecute en el servidor web, pero también se puede utilizar para hacer que un programa se ejecute en\n",
            "Respuesta esperada: Una técnica de ataque que explota fallos en la validación de entradas.\n",
            "Latencia: 1.6613 segundos\n",
            "\n",
            "Pregunta: ¿Cómo prevenir XSS?\n",
            "Respuesta generada: ¿Cómo prevenir XSS? ¿Cuales son los mejores navegadores? ¿Cuales son los mejores antivirus? ¿Cuales son los mejores firewall? ¿Cuales son los mejores navegadores web? ¿Cuales son los mejores antivirus? ¿Cuales son los mejores firewall? ¿Cuales son los mejores navegadores web? ¿Cuales son los mejores antivirus? ¿Cuales\n",
            "Respuesta esperada: Validando entradas y escapando caracteres especiales.\n",
            "Latencia: 1.6412 segundos\n",
            "\n",
            "Pregunta: ¿Qué es una vulnerabilidad CSRF?\n",
            "Respuesta generada: ¿Qué es una vulnerabilidad CSRF? Cuando un usuario acceda a un sitio web, se le solicita una contraseña de seguridad. Esta contraseña será utilizada para acceder a la información personal de los usuarios. Esta contraseña es utilizada para autenticar el usuario y para proteger los datos de los usuarios. Los datos de los usuarios son almacenados en la base de datos de la empresa y los datos de\n",
            "Respuesta esperada: Un ataque que permite realizar acciones no autorizadas en nombre de un usuario autenticado.\n",
            "Latencia: 1.8758 segundos\n",
            "\n",
            "--- Resultados del modelo bigscience/bloom-560m ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.0827\n",
            "ROUGE-L Promedio: 0.0599\n",
            "Latencia Promedio: 1.7261 segundos\n",
            "Tiempo Total: 7.94 segundos\n",
            "\n",
            "\n",
            "Evaluando modelo: PlanTL-GOB-ES/gpt2-base-bne\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: ¿Qué es una inyección SQL?\n",
            "Respuesta generada: ¿Qué es una inyección SQL? \n",
            "Respuesta esperada: Una técnica de ataque que explota fallos en la validación de entradas.\n",
            "Latencia: 0.0235 segundos\n",
            "\n",
            "Pregunta: ¿Cómo prevenir XSS?\n",
            "Respuesta generada: ¿Cómo prevenir XSS? - \n",
            "Respuesta esperada: Validando entradas y escapando caracteres especiales.\n",
            "Latencia: 0.0275 segundos\n",
            "\n",
            "Pregunta: ¿Qué es una vulnerabilidad CSRF?\n",
            "Respuesta generada: ¿Qué es una vulnerabilidad CSRF? \n",
            "Respuesta esperada: Un ataque que permite realizar acciones no autorizadas en nombre de un usuario autenticado.\n",
            "Latencia: 0.0189 segundos\n",
            "\n",
            "--- Resultados del modelo PlanTL-GOB-ES/gpt2-base-bne ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.0667\n",
            "ROUGE-L Promedio: 0.0667\n",
            "Latencia Promedio: 0.0233 segundos\n",
            "Tiempo Total: 1.71 segundos\n",
            "\n",
            "\n",
            "Evaluando modelo: google/flan-t5-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: ¿Qué es una inyección SQL?\n",
            "Respuesta generada: ¿Qué es una inyección SQL?f) a g) a f) a g) a h) a i ) a j ) )\n",
            "Respuesta esperada: Una técnica de ataque que explota fallos en la validación de entradas.\n",
            "Latencia: 1.1585 segundos\n",
            "\n",
            "Pregunta: ¿Cómo prevenir XSS?\n",
            "Respuesta generada: ¿Cómo prevenir XSS?\n",
            "Respuesta esperada: Validando entradas y escapando caracteres especiales.\n",
            "Latencia: 0.1546 segundos\n",
            "\n",
            "Pregunta: ¿Qué es una vulnerabilidad CSRF?\n",
            "Respuesta generada: ¿Qué es una vulnerabilidad CSRF?\n",
            "Respuesta esperada: Un ataque que permite realizar acciones no autorizadas en nombre de un usuario autenticado.\n",
            "Latencia: 0.1411 segundos\n",
            "\n",
            "--- Resultados del modelo google/flan-t5-base ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.0404\n",
            "ROUGE-L Promedio: 0.0404\n",
            "Latencia Promedio: 0.4847 segundos\n",
            "Tiempo Total: 2.64 segundos\n",
            "\n"
          ]
        }
      ],
      "source": [
        "models_to_test = [\n",
        "    'bigscience/bloom-560m',\n",
        "    'PlanTL-GOB-ES/gpt2-base-bne',\n",
        "    'google/flan-t5-base'\n",
        "]\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for model in models_to_test:\n",
        "    model_results = evaluate_text_generation_model(model, test_questions)\n",
        "    all_results.append(model_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euoRslBKZJA9",
        "outputId": "ce0db5a1-9d45-4685-e067-67753db96fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluando modelo: bigscience/bloom-560m por categorías\n",
            "\n",
            "Categoría: robustez\n",
            "\n",
            "Evaluando modelo: bigscience/bloom-560m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: Kómo prebenir XSS?\n",
            "Respuesta generada: Kómo prebenir XSS? - Stack Overflow en español\n",
            "Estoy usando un plugin para que mis scripts de php se actualicen en caso de que uno de mis scripts se ejecute en un proceso de una página web. El problema es que al ejecutar el script, este se ejecuta en un proceso diferente al que está programado en la página web. Por ejemplo, si ejecuto el\n",
            "Respuesta esperada: Validando entradas y escapando caracteres especiales.\n",
            "Latencia: 1.5769 segundos\n",
            "\n",
            "Pregunta: Q es un CSRF?\n",
            "Respuesta generada: Q es un CSRF? ¿Son válidas las cookies para la navegación y para el registro de la sesión?\n",
            "No. El CSRF no es necesario para la navegación y para el registro de la sesión y, por lo tanto, no afecta a la funcionalidad de los sitios web ni a la seguridad de las transacciones en línea.\n",
            "Respuesta esperada: Un ataque que permite realizar acciones no autorizadas en nombre de un usuario autenticado.\n",
            "Latencia: 1.3866 segundos\n",
            "\n",
            "Pregunta: Queé es SQL injection?\n",
            "Respuesta generada: Queé es SQL injection? Es la solución?\n",
            "Hola, tengo un proyecto y necesito saber si es posible que en el proyecto se pueda modificar el contenido de un campo de la tabla, ya que en el mismo se encuentra un campo que se llama ID, y quiero que se modifique el ID del campo ID, ya que al hacer esto me sale una consulta que dice que no puedo modificar el ID\n",
            "Respuesta esperada: Un ataque que explota fallos en la validación de entradas para ejecutar consultas maliciosas en una base de datos.\n",
            "Latencia: 1.6432 segundos\n",
            "\n",
            "Pregunta: Comóo evitar SQL INJECTION?\n",
            "Respuesta generada: Comóo evitar SQL INJECTION? - Stack Overflow en español\n",
            "Tengo un proyecto con PHP y SQL que tengo que hacer en mi sitio, pero me sale el siguiente error:\n",
            "SQL Error: 1301: The type 'int(10)' is not allowed in this syntax.\n",
            "SQL Error: 1301: The type 'int(10)' is not allowed in this syntax.\n",
            "SQL Error: 13\n",
            "Respuesta esperada: Usando sentencias preparadas y validando entradas del usuario.\n",
            "Latencia: 1.5804 segundos\n",
            "\n",
            "--- Resultados del modelo bigscience/bloom-560m ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.0875\n",
            "ROUGE-L Promedio: 0.0595\n",
            "Latencia Promedio: 1.5468 segundos\n",
            "Tiempo Total: 9.11 segundos\n",
            "\n",
            "\n",
            "Categoría: respuestas_abiertas\n",
            "\n",
            "Evaluando modelo: bigscience/bloom-560m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: ¿Por qué es importante validar entradas en una aplicación web?\n",
            "Respuesta generada: ¿Por qué es importante validar entradas en una aplicación web? ¿El uso de un certificado SSL es importante? ¿El uso de un certificado TLS es importante? ¿El uso de un certificado SSL puede hacer que una aplicación web sea más segura? ¿Cómo validar las entradas en una aplicación web? ¿El uso de un certificado SSL es importante? ¿El uso de un certificado TLS es importante? ¿El uso\n",
            "Respuesta esperada: Validar entradas previene ataques como inyecciones SQL y XSS, mejorando la seguridad de la aplicación.\n",
            "Latencia: 1.5716 segundos\n",
            "\n",
            "Pregunta: ¿Cuáles son los riesgos de no validar entradas?\n",
            "Respuesta generada: ¿Cuáles son los riesgos de no validar entradas? ¿Cómo comprobar si una entrada ha sido validada? ¿Debería validar entradas de modo que no se produzca un error?\n",
            "Respuesta esperada: La aplicación puede ser vulnerable a ataques como SQL Injection, XSS y ejecución de código remoto.\n",
            "Latencia: 0.5981 segundos\n",
            "\n",
            "Pregunta: Explique cómo funciona una vulnerabilidad de tipo CSRF.\n",
            "Respuesta generada: Explique cómo funciona una vulnerabilidad de tipo CSRF. También explica cómo se pueden explotar las vulnerabilidades de CSR.\n",
            "Con esta herramienta no solo aprenderás cómo explotar vulnerabilidades de CSRF sino también a cómo proteger tu información personal.\n",
            "Si tienes dudas sobre cómo explotar las vulnerabilidades de CSR, no dudes en contactar con nosotros.\n",
            "La empresa tecnológica de origen sueco, a través de su fundación\n",
            "Respuesta esperada: CSRF ocurre cuando un atacante induce a un usuario autenticado a ejecutar acciones no deseadas en un sistema donde está logueado, como transferencias de dinero o cambios de configuración.\n",
            "Latencia: 1.5186 segundos\n",
            "\n",
            "--- Resultados del modelo bigscience/bloom-560m ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.1290\n",
            "ROUGE-L Promedio: 0.1091\n",
            "Latencia Promedio: 1.2294 segundos\n",
            "Tiempo Total: 7.30 segundos\n",
            "\n",
            "\n",
            "Categoría: multilinguismo\n",
            "\n",
            "Evaluando modelo: bigscience/bloom-560m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: What is SQL Injection?\n",
            "Respuesta generada: What is SQL Injection? SQL Injection is a term used in the field of computer science to refer to the practice of injecting arbitrary data into a database. It is not a new practice, but it has been a great way to achieve a certain level of accuracy in the database. In recent years, the popularity of SQL Injection has increased. In recent years, the popularity of SQL In\n",
            "Respuesta esperada: A technique that exploits input validation flaws to execute malicious queries on a database.\n",
            "Latencia: 1.6690 segundos\n",
            "\n",
            "Pregunta: Como prevenir XSS?\n",
            "Respuesta generada: Como prevenir XSS? ¿Qué es XSS y cómo prevenirlo? ¿Qué ocurre si alguien accede a mi web y me pide una contraseña? ¿Qué sucede si alguien entra a mi web y me pide una contraseña? ¿Cómo puedo evitar que alguien me robe datos? ¿Cómo puedo evitar que alguien me robe datos? ¿Cómo puedo evitar que alguien me robe datos? ¿Cómo puedo evitar que alguien me robe datos?\n",
            "Respuesta esperada: Validando entradas e escapando caracteres especiais.\n",
            "Latencia: 1.7485 segundos\n",
            "\n",
            "Pregunta: Qué es un ataque de inyección SQL?\n",
            "Respuesta generada: Qué es un ataque de inyección SQL? Este es un ataque que se produce cuando un servidor SQL intenta acceder a un archivo de datos almacenado en un servidor. Hay dos tipos de ataques de inyección SQL: ataques de inyección de datos y ataques de inyección de código. El ataque de inyección de datos es un tipo de ataque que se produce cuando un servidor SQL intenta acceder a un archivo de datos almacenado\n",
            "Respuesta esperada: Un ataque que explota fallos en la validación de entradas para ejecutar consultas maliciosas en una base de datos.\n",
            "Latencia: 2.1526 segundos\n",
            "\n",
            "Pregunta: How to prevent CSRF?\n",
            "Respuesta generada: How to prevent CSRF? This is the section that should be used for the CSRF prevention. I have tried to use the following code to prevent CSRF but it doesn't work. I'm not sure what the problem is. Any help is appreciated.\n",
            "<?php\n",
            "session_start();\n",
            "session_name('csrf_token');\n",
            "session_start();\n",
            "if (isset($_GET['\n",
            "Respuesta esperada: Using CSRF tokens and verifying their validity for each user action.\n",
            "Latencia: 1.7981 segundos\n",
            "\n",
            "--- Resultados del modelo bigscience/bloom-560m ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.0939\n",
            "ROUGE-L Promedio: 0.0865\n",
            "Latencia Promedio: 1.8420 segundos\n",
            "Tiempo Total: 10.12 segundos\n",
            "\n",
            "\n",
            "Evaluando modelo: PlanTL-GOB-ES/gpt2-base-bne por categorías\n",
            "\n",
            "Categoría: robustez\n",
            "\n",
            "Evaluando modelo: PlanTL-GOB-ES/gpt2-base-bne\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: Kómo prebenir XSS?\n",
            "Respuesta generada: Kómo prebenir XSS? -El Kómo el Kómo el KSS? -El KSS? \n",
            "Respuesta esperada: Validando entradas y escapando caracteres especiales.\n",
            "Latencia: 0.1801 segundos\n",
            "\n",
            "Pregunta: Q es un CSRF?\n",
            "Respuesta generada: Q es un CSRF? que es un CSRF? \n",
            "Respuesta esperada: Un ataque que permite realizar acciones no autorizadas en nombre de un usuario autenticado.\n",
            "Latencia: 0.0826 segundos\n",
            "\n",
            "Pregunta: Queé es SQL injection?\n",
            "Respuesta generada: Queé es SQL injection? \n",
            "Respuesta esperada: Un ataque que explota fallos en la validación de entradas para ejecutar consultas maliciosas en una base de datos.\n",
            "Latencia: 0.0253 segundos\n",
            "\n",
            "Pregunta: Comóo evitar SQL INJECTION?\n",
            "Respuesta generada: Comóo evitar SQL INJECTION? - \n",
            "Respuesta esperada: Usando sentencias preparadas y validando entradas del usuario.\n",
            "Latencia: 0.0347 segundos\n",
            "\n",
            "--- Resultados del modelo PlanTL-GOB-ES/gpt2-base-bne ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.0890\n",
            "ROUGE-L Promedio: 0.0890\n",
            "Latencia Promedio: 0.0807 segundos\n",
            "Tiempo Total: 1.60 segundos\n",
            "\n",
            "\n",
            "Categoría: respuestas_abiertas\n",
            "\n",
            "Evaluando modelo: PlanTL-GOB-ES/gpt2-base-bne\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: ¿Por qué es importante validar entradas en una aplicación web?\n",
            "Respuesta generada: ¿Por qué es importante validar entradas en una aplicación web? \n",
            "Respuesta esperada: Validar entradas previene ataques como inyecciones SQL y XSS, mejorando la seguridad de la aplicación.\n",
            "Latencia: 0.0236 segundos\n",
            "\n",
            "Pregunta: ¿Cuáles son los riesgos de no validar entradas?\n",
            "Respuesta generada: ¿Cuáles son los riesgos de no validar entradas? \n",
            "Respuesta esperada: La aplicación puede ser vulnerable a ataques como SQL Injection, XSS y ejecución de código remoto.\n",
            "Latencia: 0.0189 segundos\n",
            "\n",
            "Pregunta: Explique cómo funciona una vulnerabilidad de tipo CSRF.\n",
            "Respuesta generada: Explique cómo funciona una vulnerabilidad de tipo CSRF. \n",
            "Respuesta esperada: CSRF ocurre cuando un atacante induce a un usuario autenticado a ejecutar acciones no deseadas en un sistema donde está logueado, como transferencias de dinero o cambios de configuración.\n",
            "Latencia: 0.0226 segundos\n",
            "\n",
            "--- Resultados del modelo PlanTL-GOB-ES/gpt2-base-bne ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.1568\n",
            "ROUGE-L Promedio: 0.1397\n",
            "Latencia Promedio: 0.0217 segundos\n",
            "Tiempo Total: 1.55 segundos\n",
            "\n",
            "\n",
            "Categoría: multilinguismo\n",
            "\n",
            "Evaluando modelo: PlanTL-GOB-ES/gpt2-base-bne\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: What is SQL Injection?\n",
            "Respuesta generada: What is SQL Injection? es un sistema de traducción de. \n",
            "Respuesta esperada: A technique that exploits input validation flaws to execute malicious queries on a database.\n",
            "Latencia: 0.0870 segundos\n",
            "\n",
            "Pregunta: Como prevenir XSS?\n",
            "Respuesta generada: Como prevenir XSS? que es lo correcto? \n",
            "Respuesta esperada: Validando entradas e escapando caracteres especiais.\n",
            "Latencia: 0.0622 segundos\n",
            "\n",
            "Pregunta: Qué es un ataque de inyección SQL?\n",
            "Respuesta generada: Qué es un ataque de inyección SQL? \n",
            "Respuesta esperada: Un ataque que explota fallos en la validación de entradas para ejecutar consultas maliciosas en una base de datos.\n",
            "Latencia: 0.0225 segundos\n",
            "\n",
            "Pregunta: How to prevent CSRF?\n",
            "Respuesta generada: How to prevent CSRF? - Bolso de noche, color verde - Violeta - \n",
            "Respuesta esperada: Using CSRF tokens and verifying their validity for each user action.\n",
            "Latencia: 0.1128 segundos\n",
            "\n",
            "--- Resultados del modelo PlanTL-GOB-ES/gpt2-base-bne ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.0952\n",
            "ROUGE-L Promedio: 0.0774\n",
            "Latencia Promedio: 0.0711 segundos\n",
            "Tiempo Total: 1.59 segundos\n",
            "\n",
            "\n",
            "Evaluando modelo: google/flan-t5-base por categorías\n",
            "\n",
            "Categoría: robustez\n",
            "\n",
            "Evaluando modelo: google/flan-t5-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: Kómo prebenir XSS?\n",
            "Respuesta generada: Kómo prebenir XSS?\n",
            "Respuesta esperada: Validando entradas y escapando caracteres especiales.\n",
            "Latencia: 0.1637 segundos\n",
            "\n",
            "Pregunta: Q es un CSRF?\n",
            "Respuesta generada: Q es un CSRF?ople for a cause\n",
            "Respuesta esperada: Un ataque que permite realizar acciones no autorizadas en nombre de un usuario autenticado.\n",
            "Latencia: 0.1970 segundos\n",
            "\n",
            "Pregunta: Queé es SQL injection?\n",
            "Respuesta generada: Queé es SQL injection?\n",
            "Respuesta esperada: Un ataque que explota fallos en la validación de entradas para ejecutar consultas maliciosas en una base de datos.\n",
            "Latencia: 0.1285 segundos\n",
            "\n",
            "Pregunta: Comóo evitar SQL INJECTION?\n",
            "Respuesta generada: Comóo evitar SQL INJECTION?\n",
            "Respuesta esperada: Usando sentencias preparadas y validando entradas del usuario.\n",
            "Latencia: 0.1734 segundos\n",
            "\n",
            "--- Resultados del modelo google/flan-t5-base ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.0436\n",
            "ROUGE-L Promedio: 0.0436\n",
            "Latencia Promedio: 0.1657 segundos\n",
            "Tiempo Total: 1.88 segundos\n",
            "\n",
            "\n",
            "Categoría: respuestas_abiertas\n",
            "\n",
            "Evaluando modelo: google/flan-t5-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: ¿Por qué es importante validar entradas en una aplicación web?\n",
            "Respuesta generada: ¿Por qué es importante validar entradas en una aplicación web?\n",
            "Respuesta esperada: Validar entradas previene ataques como inyecciones SQL y XSS, mejorando la seguridad de la aplicación.\n",
            "Latencia: 0.1982 segundos\n",
            "\n",
            "Pregunta: ¿Cuáles son los riesgos de no validar entradas?\n",
            "Respuesta generada: ¿Cuáles son los riesgos de no validar entradas?\n",
            "Respuesta esperada: La aplicación puede ser vulnerable a ataques como SQL Injection, XSS y ejecución de código remoto.\n",
            "Latencia: 0.5553 segundos\n",
            "\n",
            "Pregunta: Explique cómo funciona una vulnerabilidad de tipo CSRF.\n",
            "Respuesta generada: Explique cómo funciona una vulnerabilidad de tipo CSRF.\n",
            "Respuesta esperada: CSRF ocurre cuando un atacante induce a un usuario autenticado a ejecutar acciones no deseadas en un sistema donde está logueado, como transferencias de dinero o cambios de configuración.\n",
            "Latencia: 0.3118 segundos\n",
            "\n",
            "--- Resultados del modelo google/flan-t5-base ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.1568\n",
            "ROUGE-L Promedio: 0.1397\n",
            "Latencia Promedio: 0.3551 segundos\n",
            "Tiempo Total: 2.60 segundos\n",
            "\n",
            "\n",
            "Categoría: multilinguismo\n",
            "\n",
            "Evaluando modelo: google/flan-t5-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM'].\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pregunta: What is SQL Injection?\n",
            "Respuesta generada: What is SQL Injection?\n",
            "Respuesta esperada: A technique that exploits input validation flaws to execute malicious queries on a database.\n",
            "Latencia: 0.1066 segundos\n",
            "\n",
            "Pregunta: Como prevenir XSS?\n",
            "Respuesta generada: Como prevenir XSS?\n",
            "Respuesta esperada: Validando entradas e escapando caracteres especiais.\n",
            "Latencia: 0.1537 segundos\n",
            "\n",
            "Pregunta: Qué es un ataque de inyección SQL?\n",
            "Respuesta generada: Qué es un ataque de inyección SQL?\n",
            "Respuesta esperada: Un ataque que explota fallos en la validación de entradas para ejecutar consultas maliciosas en una base de datos.\n",
            "Latencia: 0.3436 segundos\n",
            "\n",
            "Pregunta: How to prevent CSRF?\n",
            "Respuesta generada: How to prevent CSRF?d by avoiding exposure to the elements.\n",
            "Respuesta esperada: Using CSRF tokens and verifying their validity for each user action.\n",
            "Latencia: 0.2955 segundos\n",
            "\n",
            "--- Resultados del modelo google/flan-t5-base ---\n",
            "BLEU Promedio: 0.0000\n",
            "ROUGE-1 Promedio: 0.0942\n",
            "ROUGE-L Promedio: 0.0763\n",
            "Latencia Promedio: 0.2249 segundos\n",
            "Tiempo Total: 2.56 segundos\n",
            "\n"
          ]
        }
      ],
      "source": [
        "models_to_test = [\n",
        "    'bigscience/bloom-560m',\n",
        "    'PlanTL-GOB-ES/gpt2-base-bne',\n",
        "    'google/flan-t5-base'\n",
        "]\n",
        "\n",
        "all_results_categories = []\n",
        "\n",
        "for model in models_to_test:\n",
        "    model_results = evaluate_models_with_categories(model, test_questions_categories)\n",
        "    all_results_categories.append(model_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62RmiP2qBbBv"
      },
      "source": [
        "### Guardar resultados en un informe\n",
        "Los resultados de la evaluación se guardan en un archivo JSON llamado `evaluation_results.json` y `evaluation_results_categories.json` respectivamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOwIPbB6BbBv",
        "outputId": "5a0208b2-590a-4d88-d17d-c4bf11ec9e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados guardados en evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "output_file = 'evaluation_results.json'\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(all_results, f, indent=4)\n",
        "\n",
        "print(f'Resultados guardados en {output_file}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRQL4WPsZwVV",
        "outputId": "0fcfe4c2-0ed9-488e-8f2e-e4e55c1f5353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados guardados en evaluation_results.json\n"
          ]
        }
      ],
      "source": [
        "output_file_categories = 'evaluation_results_categories.json'\n",
        "with open(\"evaluation_results_categories.json\", \"w\") as f:\n",
        "    json.dump(all_results_categories, f, indent=4)\n",
        "\n",
        "print(f'Resultados guardados en {output_file}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}